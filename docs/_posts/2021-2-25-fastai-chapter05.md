---
layout: post
title: สรุป course.fast.ai (part1 v4) คาบที่ 5
---

Deep Learning เป็นเครื่องมือที่ทรงพลัง ดังนั้นเราจำเป็นต้องคำนึงถึงผลกระทบจากโมเดลที่เราสร้างด้วย
เรามีความตระหนักในฐานะนักพัฒนาเพียงพอแล้วหรือยังว่า สิ่งที่เราพัฒนาสามารถส่งผลกระทบต่อสังคมได้?

# จริยธรรม (Ethics) คือ อะไร
- หลัก หรือ มาตรฐาน ของการกระทำที่ถูกต้อง และ ไม่ถูกต้อง เป็นหลักการที่ชี้แนะว่าเราควรจะทำอะไร ไม่ควรทำอะไร
-  การศึกษา และ พัฒนาหลักจริยธรรมในแต่ละบุคคล

# ปัญหาเชิงจริยธรรมด้านต่างๆ

## กระบวนการแก้ปัญหา และความรับผิดชอบ
- เป็นเรื่องปกติที่จะมีความผิดพลาด (error) ในข้อมูล
- Algorithm หรือ Software ก็สามารถมี bug ได้เช่นกัน (ความผิดพลาดในเชิง software)
- กรณีศึกษา: ในประเทศสหรัฐอเมริกา มีการใช้ software เพื่อพิจารณางบประมาณในส่วนของประกันสุขภาพ จากการตรวจสอบของ The Verge พบว่าในเมือง Arkansas มีผู้ได้รับผลกระทบถูกตัดงบประมาณประกันสุขภาพโดยเฉพาะในกลุ่มผู้ป่วย เบาหวาน (diabetes) และ ภาวะพิการทางสมอง (celebral palsy) สาเหตุมาจากความผิดพลาดในการนำ algorithm มาใช้ ["What Happens When an Algorithm Cuts Your Healthcare"](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy)
- เราควรจะมีกระบวนการสำหรับตรวจสอบ software และ การแก้ไขข้อผิดพลาดหรือไม่

## Feedback loops
- จะเกิดอะไรขึ้น เมื่อโมเดลกำหนดข้อมูลที่จะถูกนำมาใช้ฝึกโมเดลในรอบถัดไป จากผลลัพธ์การทำนายในรอบปัจจุบัน?
- ทิศทางที่โมเดลทำนาย หรือแนะนำ จะยิ่งส่งผลต่อการทำนาย หรือแนะนำในอนาคต ให้มาในทิศทางเดิมเรื่อยๆ จากข้อจำกัดในตัวเลือก หรือผลลัพธ์ในการทำนาย หรือแนะนำในแต่ละครั้ง เช่น บน Facebook ยิ่งเราดู หรือ กด like กับเนื้อหาประเภทไหนเยอะๆ ก็จะยิ่งได้เนื้อหาลักษณะนั้นมาบน news feed จะเกิดอะไรขึ้นหากเราเริ่มเข้าไปบริโภคข้อมูลที่เป็นเพียงการคาดเดา และถูกจงใจสร้างมาเพื่อชวนเชื่อ และ สร้างความแตกแยก?
- กรณีศึกษา: ระบบแนะนำวิดีโอของ Youtube ถูกออกแบบมาให้ผู้ใช้งานดูวิดีโอ และใช้งานอยู่บน platform นานๆ ยิ่งผู้ใช่้งานมีประวัติการดูวิดีโอประเภทใดประเภทหนึ่งมากเท่าไหร่ ก็จะยิ่งได้รับวิดีโอแนะนำประเภทนั้นมากเท่านั้น สามารถทำให้เกิดพฤติกรรมติดได้ นอกเหนือจากนี้ วิดีโอที่ได้รับความนิยม และถูกแนะนำมากๆ ก็จะยิ่งได้รับความนิยม เนื่องจาก YouTube ไม่สามารถควบคุมเนื้อหาของวิดีโอทั้งหมดได้ ในบางครั้งวิดีโอเหล่านี้อาจจะเป็นข้อมูลที่ยังไม่ได้รับการพิสูจน์ การคาดเดา หรือ ทฤษฎีสมคบคิด ก็เป็นได้ ซึ่งก็มีโอกาสที่ระบบแนะนำวิดีโอจะแนะนำวิดีโอเหล่านี้กับคนจำนวนมาก เกิดการบริโภคข้อมูลผิดๆ และนำไปสู่ความขัดแย้งได้ ["YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?"](https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html)
- กรณีศึกษา: เด็กผุ้หญิง 10 ขวบ และเพื่อนของเธออัพโหลดวิดีโอที่พวกเขาเล่นกันที่สระน้ำในสวนหลังบ้านลง YouTube และ ไม่กี่วันถัดมา ยอด view ของวิดีโอดังกล่าวก็ขึ้นไปถึง 400,000 ทำให้ผู้ปครองของเด็กตกใจ และ แปลกใจเป็นอย่างมาก หลังจากการตรวจสอบจึงพบว่า วิดีโิดังกล่าวถูกแนะนำให้กับกลุ่มที่มีรสนิยามทางเพศกับเด็ก (paedophile) ["On YouTube’s Digital Playground, an Open Gate for Pedophiles"](https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html)
- กรณีศึกษา: Evan Estola lead machine learning engineer บริษัท Meetup
ตัดสินใจไม่ใช้ เพศ​ ของผู้ใช้งานมาเป็นตัวแปรในระบบแนะนำ tech meetup ให้กับผู้ใช้งาน เนื่องจากโดยปกติแล้วผู้ชายมักจะให้ความสนใจกับ meetup ประเภทนี้มากกว่าผู้หญิงอยู่แล้ว หากมีการนำ เพศ​ มาเป็นตัวแปรในโมเดลสำหรับแนะนำ tech meetup ก็จะยิ่งทำให้โมเดลแนะนำ tech meetup ให้ผู้หญิงน้อย และส่งผลให้ผู้หญิงเข้าร่วม tech meetup น้อยลงไปอีก และ เกิดเป็น feedback loops ส่งผลให้ผู้หญิงเข้าร่วม tech meetup น้อยลงไปเรื่อยๆ

## Bias
- กรณีศึกษา: Dr. Latanya Sweeney พบว่าโฆษณาใน Google ที่แสดงขึ้นมาเวลาค้นหาด้วยชื่อของคนผิวดำ จะมีลักษณะเอนเอียงไปในทางเสนอแนะว่าบุคคลนั้นมีประวัติอาชญากรรม ในขณะที่ถ้าค้นหาด้วยชื่อคนผิวขาว โฆษณาจะค่อนข้างเป็นกลางมากกว่า ["Discrimination in Online Ad Delivery"](https://arxiv.org/abs/1301.6822)
- Historical bias: ความเอนเอียงที่ฝังรากอยู่ในประวัติศาสตร์ เช่น การแบ่งแยกเชื้อชาติ และ สีผิว ที่ส่งผลให้เกิดการปฏิบัติที่ไม่เท่าเทียม หากเรานำเอาข้อมูลที่มีความไม่เท่าเทียมนี้อยู่มาใช้ ก็เป็นการสนับสนุนให้เกิดความไม่เท่าเทียมกันต่อไป
- กรณีศึกษา: COMPAS Algorithm ที่ถูกใช้ในสหรัฐอเมริกา สำหรับตัดสินจำคุก หรือประกันตัวจำเลย ถูกตรวจสอบโดย Propulica และพบว่ามีแนวโน้มในการตัดสินจำคุกคนผิวดำมากกว่าคนผิวขาว อย่างมีนัยสำคัญ ["Machine Bias-Risk Assessments in Criminal Sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
- กรณีศึกษา: กลุ่มนักวิจัยจาก MIT ทำการศึกษาความแม่นยำของระบบจำแนกเพศผ่านการตรวจสอบใบหน้า ในกลุ่มสีผิวต่างๆ พบว่าความแม่นยำในการจำแนกเพศของผู้หญิงที่มีผิวสีอยู่ในระดับต่ำ เมืื่อเทียบกับความแม่นยำในการจำแนกเพศในกลุ่มอื่นๆ หรือตัวเลขความแม่นยำโดยรวมที่ถูกใช้ในการรายงานประสิทธิภาพของระบบ
![](https://github.com/fastai/fastbook/blob/master/images/ethics/image9.jpeg?raw=1)
- Measurement bias: สิ่งที่เราวัดมีความเอนเอียง หรือไม่ หรือข้อมูลที่เราเก็บมามีความเอนเอียงไปยังประชากรเพียงบางกลุ่มจากทั้งหมดหรือไม่
- กรณีศึกษา: จากการศึกษาโมเดลทำนายการเกิดโรคหลอดเลือดสมอง (stroke) จากประวัติการตรวจโรคของคนไข้ ของ Sendhil Mullainathan และ Ziad Obermeyer โดยอ้างอิงจากโมเดล ปัจจัยหลักที่ส่งผลต่อการทำนายว่าจะเกิดโรคหลอดเลือดสมองได้แก่
    - Prior stroke
    - Cardiovascular disease
    - Accidental injury
    - Benign breast lump
    - Colonoscopy
    - Sinusitis
อย่างไรก็ตาม มีเพียงสองปัจจัยแรกเท่านั้นที่ส่งผลต่อการเกิดโรคหลอดเลือดสมองจริงๆ ในขณะที่ปัจจัยอื่นๆ เป็นเพียง correlation กับการมาตรวจโรคของอาการอื่นๆ ที่ไม่เกี่ยวข้อง ของคนไข้ที่มีสิทธิ์ หรือมีความสามารถเข้าถึงการรักษาเท่านั้น ไม่ได้เป็นสาเหตุของการเกิดโรคแต่อย่างใด
("Does Machine Learning Automate Moral Hazard and Error")[https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf]
- Aggregation bias: ข้อมูลสามารถถูกนำมาสรุปเชิงสถิติจนสูญเสียความสัมพันธ์ หรือข้อมูลที่สำคัฐระหว่างตัวแปรไปได้
- Representation bias: ในกรณีที่ไม่มีความ balance ระหว่าง class ในข้อมูล ML โมเดลจะยิ่งขยายความไม่ balance ระหว่าง class นี้ ในผลการทำนายมากขึ้นไป ![](https://github.com/fastai/fastbook/blob/master/images/ethics/image12.png?raw=1)

## Disinformation
- Disinformation ไม่ได้จำกัดอยู่แค่การให้ข้อมูลที่ไม่เป็นความจริง หรือ ข่าวปลอม (fake news) เท่านั้น แต่รวมไปถึงการใช้ข้อมูลที่มีความจริงบางส่วน ที่ถูกนำมาวางในบริบทผิดๆ หรือปะปนกับข้อมูลเท็จ หรือข้อมูลเกินจริง เพื่อให้เกิดความสับสน และขัดขวางการแสวงหาข้อเท็จจริงของผู้รับสาร ก็ได้ เช่น การใช้ account ปลอมเพื่อพยามสร้างอิทธิพล หรือกระแสในการแสดงความคิดเห็น ให้เกิดการเห็นตาม เพราะความคิดเห็นของเราสามารถถูกชักจูงโดยพฤติกรรมหมู่ได้เสมอ
- นอกเหนือจากนี้ เรายังสามารถใช้ Deep Learning ในการสร้างข้อความ หรือรูปภาพ ที่เสมือนจริงขึ้นมาได้ ทำให้เราต้องระมัดระวังรูปแบบการนำไปใช้ที่ผิดๆ หรือหวังผลประโยชน์แฝง
- แนวทางแก้ปัญหาทางหนึ่งที่ถูกเสนอ ได้แก่ การสร้าง Digital Signature สำหรับยืนยัน และรับร้องเนื้อหา จากแหล่งข้อมูลที่ได้รับการรับรองแล้วเท่านั้น ["How Will We Prevent AI-Based Forgery?"]()

# ทำไมเราต้องสนใจปัญหาเชิงจริยธรรม
- ในฐานะ data scientist เราจะสนใจเพียงแค่การ optimize โมเดลให้ได้ค่า metric ที่ดีเท่านั้นหรือไม่? หรือเราควรสนใจด้วยว่าโมเดลจะถูกนำไปใช้เพื่อใคร อย่างไร และจะส่งผลกระทบอะไรหรือไม่?
- กรณีศึกษา: ในช่วงสงครามโลกครั้งที่ 2 IBM พัฒนาผลิตภัณฑ์สำหรับติดตามจำนวนคนยิว และ กลุ่มอื่นๆ ที่ถูกฆ่าในค่ายกักกัน ให้กับกลุ่ม Nazi รวมถึงจัดสรรการดูแลอุปกรณ์ (maintenance) และ สอนการใช้งาน (training) ในกรณีดังกล่าวนักวิทยาศาสตร์ผู้สร้างเทคโนโลยี หรืออุปกรณ์เหล่านี้ ควรมีส่วนรับผิดชอบกับการนำไปใช้งานในลักษณะนี้หรือไม่? จะมีความแตกต่างอะไรหรือไม่ ถ้านักวิทยาศาสตร์ที่เป็นผู้สร้างเทคโนโลยี ตั้งคำถามเกี่ยวกับการนำไปใช้ และปฏิเสธการพัฒนาที่นำไปสู่การนำไปใช้ที่ผิดๆ

# การระบุ และจัดการปัญหาเชิงจริยธรรม
ขั้นตอนการจัดการกับปัญหาเชิงจริยธรรม
## วิเคราะห์ project ที่ทำอยู่
    ตั้งคำถามเหล่านี้กับ project ที่ทำอยู่
    - เราทำสิ่งที่ควรทำอยู่ จริงหรือไม่?
    - ข้อมูลมี bias อะไรอยู่บ้าง?
    - สามารถตรวจสอบ code และ ข้อมูล เพื่อป้องกัน หรือแก้ไข bug/ error ได้ไหม?
    - metric เช่น accuracy/ error rate มีความแตกต่างกันใน target กลุ่มต่างๆ หรือไม่?
    - accuracy ของ baseline เป็นอย่างไร?
    - กระบวนการแก้ปัญหาเมื่อมีการร้องเรียน หรือเกิด bug/ error หลังจากโมเดลถูกใช้งานแล้ว มีขั้นตอนอย่างไร?
    - ทีมผู้สร้างมีความหลากหลายมากน้อยแค่ไหน?
## วางขั้นตอนจัดการกับปัญหาเชิงจริยธรรม
- Markkula Center ได้จัดทำหลักปฏิบัติเพื่อจัดการกับปัญหาเชิงจริยธรรม ซึ่งครอบคลุมไปถึง การตรวจสอบหาปัญหา และความเสี่ยง, การขยายวงผู้รับผิดชอบกับปัญหาเชิงจริยธรรมให้ครอบคลุม, และการพิจารณาความเสี่ยงจากกลุ่มบุคคลที่มีจุดประสงค์มุ่งร้าย หรือสามารถใช้ประโยชน์จาก project ในทางที่ไม่ถูกต้อง ["An Ethical Toolkit for Engineering/Design Practice"](https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/)
## สร้างทีมที่มีความหลากหลาย
- โดยปกติเรามักจะหาคนที่มี background คล้ายๆ กับเรามาร่วมทีม หรือทำ project ร่วมกัน ด้วยความเข้ากันได้ และ การมี bias กับลักษณะนิสัย หรือพื้นฐาน ที่คล้ายกับเรา อย่างไรก็ตามการที่ทีมขาดความหลากหลาย มักจะทำให้เกิดความเสี่ยงจากการมองข้ามจริยธรรมในรูปแบบคล้ายๆกัน ทำให้ความคิดเห็นและการตัดสินใจเป็นไปในทิศทางเดียว
## การใช้นโยบาย และการควบคุม หรือ กฏเข้าช่วย
- นอกเหนือจากขั้นตอนการจัดการปัญหาเชิงจริยธรรมที่ยกมาก่อนหน้านี้แล้ว การวางนโยบาย และข้อบังคับ ก็มีส่วนสำคัญในการจัดการกับปัญหาเชิงจริยธรรมอย่างครอบคลุม โดยเฉพาะเมื่อมีการกำหนดโทษ​ หรือค่าปรับ (financial penalty) ก็สามารถเป็นปัจจัยกระตุ้นให้เกิดการวางนโยบาย หรือการแก้ไขปัญหาเชิงจริยธรรมได้ ["Facebook hiring hundreds to comply with hate speech law"](https://thehill.com/policy/technology/361722-facebook-opens-second-german-office-to-comply-with-hate-speech-law)

ตอบคำถามท้ายบทได้ที่ [aiquizzes](https://aiquizzes.com/howto)